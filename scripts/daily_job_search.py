"""
GitHub Actionsì—ì„œ ì‹¤í–‰ë˜ëŠ” ì¼ì¼ ì±„ìš© ê³µê³  ê²€ìƒ‰ ìŠ¤í¬ë¦½íŠ¸
"""
import os
import sys
import json
from datetime import datetime
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))

from app.services.job_search import JobSearchService
from app.models.job import JobSearchRequest

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ê²€ìƒ‰ í‚¤ì›Œë“œ ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” ê¸°ë³¸ê°’)
    keywords = os.getenv('SEARCH_KEYWORDS', 'Python Developer,Software Engineer').split(',')
    location = os.getenv('SEARCH_LOCATION', 'Seoul, South Korea')
    
    # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
    jobs_dir = Path(__file__).parent.parent / 'jobs'
    jobs_dir.mkdir(exist_ok=True)
    
    all_jobs = []
    search_service = JobSearchService()
    
    print(f"ğŸ” ì±„ìš© ê³µê³  ê²€ìƒ‰ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # ë¹„ë™ê¸° ì‹¤í–‰ì„ ìœ„í•œ ë˜í¼
    import asyncio
    
    async def search_all_keywords():
        """ëª¨ë“  í‚¤ì›Œë“œë¡œ ë¹„ë™ê¸° ê²€ìƒ‰"""
        tasks = []
        for keyword in keywords:
            keyword = keyword.strip()
            if not keyword:
                continue
                
            print(f"\nğŸ“Œ ê²€ìƒ‰ í‚¤ì›Œë“œ: {keyword}")
            
            request = JobSearchRequest(
                keyword=keyword,
                location=location,
                max_results=30,
                sources=['linkedin', 'indeed']
            )
            
            tasks.append(search_service.search_jobs(request))
        
        # ëª¨ë“  ê²€ìƒ‰ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        jobs_list = []
        for idx, result in enumerate(results):
            if isinstance(result, list):
                print(f"âœ… {len(result)}ê°œì˜ ì±„ìš© ê³µê³  ë°œê²¬")
                jobs_list.extend(result)
            elif isinstance(result, Exception):
                print(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {result}")
        
        return jobs_list
    
    all_jobs = asyncio.run(search_all_keywords())
    
    # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
    seen_urls = set()
    unique_jobs = []
    for job in all_jobs:
        if job.url not in seen_urls:
            seen_urls.add(job.url)
            unique_jobs.append(job)
    
    print(f"\nğŸ“Š ì´ {len(unique_jobs)}ê°œì˜ ê³ ìœ í•œ ì±„ìš© ê³µê³  ë°œê²¬")
    
    # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_file = jobs_dir / f'jobs_{timestamp}.json'
    
    # Pydantic ëª¨ë¸ì„ dictë¡œ ë³€í™˜
    jobs_data = [job.model_dump() for job in unique_jobs]
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            'timestamp': datetime.now().isoformat(),
            'total': len(unique_jobs),
            'keywords': keywords,
            'location': location,
            'jobs': jobs_data
        }, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_file}")
    
    # ìµœì‹  ê²°ê³¼ë¥¼ latest.jsonìœ¼ë¡œë„ ì €ì¥
    latest_file = jobs_dir / 'latest.json'
    with open(latest_file, 'w', encoding='utf-8') as f:
        json.dump({
            'timestamp': datetime.now().isoformat(),
            'total': len(unique_jobs),
            'keywords': keywords,
            'location': location,
            'jobs': jobs_data
        }, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ ìµœì‹  ê²°ê³¼ ì €ì¥: {latest_file}")
    
    # README ì—…ë°ì´íŠ¸
    update_readme(unique_jobs, keywords, location)
    
    print("\nâœ… ì‘ì—… ì™„ë£Œ!")
    print(f"\nğŸ’¡ ê²°ê³¼ í™•ì¸ ë°©ë²•:")
    print(f"   1. JSON íŒŒì¼: {latest_file}")
    print(f"   2. ì½˜ì†” ì¶œë ¥: python scripts/view_results.py")
    print(f"   3. HTML ë·°ì–´: python scripts/view_results.py --html")

def update_readme(jobs, keywords, location):
    """README íŒŒì¼ì— ìµœì‹  ì±„ìš© ê³µê³  ì •ë³´ ì—…ë°ì´íŠ¸"""
    readme_path = Path(__file__).parent.parent / 'README.md'
    
    if not readme_path.exists():
        return
    
    with open(readme_path, 'r', encoding='utf-8') as f:
        readme_content = f.read()
    
    # ìµœì‹  ì±„ìš© ê³µê³  ì„¹ì…˜ ìƒì„±
    timestamp = datetime.now().strftime('%Yë…„ %mì›” %dì¼ %H:%M')
    
    jobs_section = f"""
## ğŸ“‹ ìµœì‹  ì±„ìš© ê³µê³  (ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {timestamp})

**ê²€ìƒ‰ í‚¤ì›Œë“œ**: {', '.join(keywords)}  
**ì§€ì—­**: {location}  
**ì´ {len(jobs)}ê°œì˜ ì±„ìš© ê³µê³ **

### ìƒìœ„ 10ê°œ ì±„ìš© ê³µê³ 

"""
    
    for idx, job in enumerate(jobs[:10], 1):
        jobs_section += f"""
{idx}. **{job.title}** - {job.company}
   - ğŸ“ {job.location or 'ìœ„ì¹˜ ì •ë³´ ì—†ìŒ'}
   - ğŸ”— [ìì„¸íˆ ë³´ê¸°]({job.url})
   - ì¶œì²˜: {job.source}
   
"""
    
    if len(jobs) > 10:
        jobs_section += f"\n*ê·¸ ì™¸ {len(jobs) - 10}ê°œì˜ ì±„ìš© ê³µê³ ê°€ ë” ìˆìŠµë‹ˆë‹¤. [ì „ì²´ ëª©ë¡ ë³´ê¸°](jobs/latest.json)*\n"
    
    # ê¸°ì¡´ ìµœì‹  ì±„ìš© ê³µê³  ì„¹ì…˜ ì°¾ì•„ì„œ êµì²´
    import re
    pattern = r'## ğŸ“‹ ìµœì‹  ì±„ìš© ê³µê³ .*?(?=\n## |\Z)'
    
    if re.search(pattern, readme_content, re.DOTALL):
        readme_content = re.sub(pattern, jobs_section.strip(), readme_content, flags=re.DOTALL)
    else:
        # ì„¹ì…˜ì´ ì—†ìœ¼ë©´ ì„¤ì¹˜ ë° ì‹¤í–‰ ì„¹ì…˜ ì•ì— ì¶”ê°€
        install_pattern = r'(## ì„¤ì¹˜ ë° ì‹¤í–‰)'
        if re.search(install_pattern, readme_content):
            readme_content = re.sub(
                install_pattern,
                jobs_section.strip() + '\n\n' + r'\1',
                readme_content
            )
        else:
            # ëì— ì¶”ê°€
            readme_content += '\n\n' + jobs_section
    
    with open(readme_path, 'w', encoding='utf-8') as f:
        f.write(readme_content)
    
    print("ğŸ“ README ì—…ë°ì´íŠ¸ ì™„ë£Œ")

if __name__ == '__main__':
    main()

